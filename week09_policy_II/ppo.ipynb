{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9dfWABRNCVm",
    "outputId": "ee8f0e74-ab77-4b9e-ff10-05d4e7a0930b"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    \n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/runners.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/mujoco_wrappers.py\n",
    "    \n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI0kqjVINCVt"
   },
   "source": [
    "# Implementing Proximal Policy Optimization \n",
    "\n",
    "\n",
    "In this notebook you will be implementing Proximal Policy Optimization algorithm, \n",
    "scaled up version of which was used to train [OpenAI Five](https://openai.com/blog/openai-five/) \n",
    "to [win](https://openai.com/blog/how-to-train-your-openai-five/) against the\n",
    "world champions in Dota 2.\n",
    "You will be solving a continuous control environment on which it may be easier and faster \n",
    "to train an agent, however note that PPO here may not be the best algorithm as, for example,\n",
    "Deep Deterministic Policy Gradient and Soft Actor Critic may be more suited \n",
    "for continuous control environments. To run the environment you will need to install \n",
    "[pybullet-gym](https://github.com/benelot/pybullet-gym) which unlike MuJoCo \n",
    "does not require you to have a license.\n",
    "\n",
    "To install the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31MBortONCVv",
    "outputId": "13ea5dac-6194-497a-8ca3-d7bde217798c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'lib/pybullet-gym' already exists and is not an empty directory.\n",
      "Obtaining file:///Users/fritz/playground/lib/pybullet-gym\n",
      "Requirement already satisfied: pybullet>=1.7.8 in /Users/fritz/anaconda3/lib/python3.8/site-packages (from pybulletgym==0.1) (3.2.0)\n",
      "Installing collected packages: pybulletgym\n",
      "  Attempting uninstall: pybulletgym\n",
      "    Found existing installation: pybulletgym 0.1\n",
      "    Uninstalling pybulletgym-0.1:\n",
      "      Successfully uninstalled pybulletgym-0.1\n",
      "  Running setup.py develop for pybulletgym\n",
      "Successfully installed pybulletgym\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/benelot/pybullet-gym lib/pybullet-gym\n",
    "!pip install -e lib/pybullet-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MZeKC7aO4RIC"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iod_fZUFNCVw"
   },
   "source": [
    "The overall structure of the code is similar to the one in the A2C optional homework, but don't worry if you haven't done it, it should be relatively easy to figure it out. \n",
    "First, we will create an instance of the environment. \n",
    "We will normalize the observations and rewards, but before that you will need a wrapper that will \n",
    "write summaries, mainly, the total reward during an episode. You can either use one for `TensorFlow` \n",
    "implemented in `atari_wrappers.py` file from the optional A2C homework, or implement your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2N76WIRNCVx",
    "outputId": "b23c69ce-eedb-4043-9f8c-1d89eec416a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WalkerBase::__init__\n",
      "observation space:  Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32) \n",
      "observations: [-0.02690491 -0.05298987  0.09801119 -0.00255291 -0.04580725 -0.02988132\n",
      " -0.05721294  0.02274642  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.        ]\n",
      "action space:  Box([-1. -1. -1. -1. -1. -1.], [1. 1. 1. 1. 1. 1.], (6,), float32) \n",
      "action_sample:  [ 0.8777564   0.7727072   0.90788525 -0.87308174  0.48898658 -0.7937719 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fritz/anaconda3/lib/python3.8/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import pybulletgym\n",
    "\n",
    "env = gym.make(\"HalfCheetahMuJoCoEnv-v0\")\n",
    "print(\"observation space: \", env.observation_space,\n",
    "      \"\\nobservations:\", env.reset())\n",
    "print(\"action space: \", env.action_space, \n",
    "      \"\\naction_sample: \", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_au8nSY479qw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Summaries(gym.Wrapper):\n",
    "    \"\"\" Wrapper to write summaries. \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.episode_counter = 0\n",
    "        self.current_step_var = 0\n",
    "\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lens = []\n",
    "\n",
    "        self.current_reward = 0\n",
    "        self.current_len = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "\n",
    "        self.current_reward += rew\n",
    "        self.current_len += 1\n",
    "        self.current_step_var += 1\n",
    "\n",
    "        if done:\n",
    "            self.episode_rewards.append((self.current_step_var, self.current_reward))\n",
    "            self.episode_lens.append((self.current_step_var, self.current_len))\n",
    "\n",
    "        return obs, rew, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.episode_counter += 1\n",
    "\n",
    "        self.current_reward = 0\n",
    "        self.current_len = 0\n",
    "\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVom_S9yNCVz"
   },
   "source": [
    "The normalization wrapper will subtract running mean from observations and rewards and divide \n",
    "the resulting quantities by the  running variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxevKC-FNCVz",
    "outputId": "11bcf7dd-b04c-47c8-c8e0-ca8724aee842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WalkerBase::__init__\n"
     ]
    }
   ],
   "source": [
    "from mujoco_wrappers import Normalize\n",
    "\n",
    "env = Normalize(Summaries(gym.make(\"HalfCheetahMuJoCoEnv-v0\")));\n",
    "env.unwrapped.seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9lf2Z8bNCV0"
   },
   "source": [
    "Next, you will need to define a model for training. We suggest that you use two separate networks: one for policy\n",
    "and another for value function. Each network should be a 3-layer MLP with 64 hidden units, $\\mathrm{tanh}$ \n",
    "activation function.\n",
    "\n",
    "Our policy distribution is going to be multivariate normal with diagonal covariance. \n",
    "The network from above will predict the mean, and the covariance should be represented by a single \n",
    "(learned) vector of size 6 (corresponding to the dimensionality of the action space from above). \n",
    "You should initialize this vector to zero and take the exponent of it to always\n",
    "have a non-negative quantity. \n",
    "\n",
    "Overall the model should return three things: predicted mean of the distribution, variance vector, \n",
    "value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OwwLcUApNCV0"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "class PolicyModel(nn. Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyModel, self).__init__()\n",
    "        self.h = 64\n",
    "\n",
    "        self.policy_model = < Create your model >\n",
    "\n",
    "        self.value_model = < Create your model >\n",
    "\n",
    "    def get_policy(self, x):\n",
    "        < insert your code here >\n",
    "        return means, var\n",
    "\n",
    "    def get_value(self, x):\n",
    "        out = self.value_model(x.float())\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        policy = self.get_policy(x)\n",
    "        value = self.get_value(x)\n",
    "\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI6d9Xq_NCV1"
   },
   "source": [
    "This model will be wrapped by a `Policy`. The policy can work in two modes, but in either case \n",
    "it is going to return dictionary with string-type keys. The first mode is when the policy is \n",
    "used to sample actions for a trajectory which will later be used for training. In this case \n",
    "the flag `training` passed to `act` method is `False` and the method should return \n",
    "a `dict` with the following keys: \n",
    "\n",
    "* `\"actions\"`: actions to pass to the environment\n",
    "* `\"log_probs\"`: log-probabilities of sampled actions\n",
    "* `\"values\"`: value function $V^\\pi(s)$ predictions.\n",
    "\n",
    "We don't need to use the values under these keys for training, so all of them should be of type `np.ndarray`.\n",
    "\n",
    "When `training` is `True`, the model is training on a given batch of observations. In this\n",
    "case it should return a `dict` with the following keys\n",
    "\n",
    "* `\"distribution\"`: an instance of multivariate normal distribution (`torch.distributions.MultivariateNormal` or `tf.distributions.MultivariateNormalDiag`)\n",
    "* `\"values\"`: value function $V^\\pi(s)$ prediction.\n",
    "\n",
    "The distinction about the modes comes into play depending on where the policy is used: if it is called from `EnvRunner`, \n",
    "the `training` flag is `False`, if it is called from `PPO`, the `training` flag is `True`. These classed \n",
    "will be described below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C54tGZahXbSa"
   },
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def act(self, inputs, training=False):\n",
    "        inputs = torch.tensor(inputs)\n",
    "        if inputs.ndim < 2:\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "        inputs = inputs.cuda()\n",
    "        \n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        < insert your code here >\n",
    "        normal_distr = MultivariateNormal(means, covar_matrix)\n",
    "\n",
    "        actions = normal_distr.sample()\n",
    "        log_probs = normal_distr.log_prob(actions)\n",
    "\n",
    "        values = self.model.get_value(inputs)\n",
    "\n",
    "        if not training:\n",
    "            return {'actions': actions.cpu().numpy().tolist()[0], \n",
    "                    'log_probs': log_probs[0].detach().cpu().numpy(),\n",
    "                    'values': values[0].detach().cpu().numpy()}\n",
    "        else:\n",
    "            return {'distribution': normal_distr, 'values': values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kML-jSsNCV2"
   },
   "source": [
    "We will use `EnvRunner` to perform interactions with an environment with a policy for a fixed number of timesteps. Calling `.get_next()` on a runner will return a trajectory &mdash; dictionary \n",
    "containing keys\n",
    "\n",
    "* `\"observations\"`\n",
    "* `\"rewards\"` \n",
    "* `\"resets\"`\n",
    "* `\"actions\"`\n",
    "* all other keys that you defined in `Policy`,\n",
    "\n",
    "under each of these keys there is a `np.ndarray` of specified length $T$ &mdash; the size of partial trajectory. \n",
    "\n",
    "Additionally, before returning a trajectory this runner can apply a list of transformations. \n",
    "Each transformation is simply a callable that should modify passed trajectory in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5LeiuohcNCV3"
   },
   "outputs": [],
   "source": [
    "class AsArray:\n",
    "    \"\"\" \n",
    "    Converts lists of interactions to ndarray.\n",
    "    \"\"\"\n",
    "    def __call__(self, trajectory):\n",
    "        # Modify trajectory inplace. \n",
    "        for k, v in filter(lambda kv: kv[0] != \"state\", trajectory.items()):\n",
    "            trajectory[k] = np.asarray(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNzPNxKLNCV4",
    "outputId": "e15b29e7-1864-45db-e67a-3f1994b77a81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': (3, 6),\n",
       " 'values': (3,),\n",
       " 'observations': (3, 17),\n",
       " 'rewards': (3,),\n",
       " 'resets': (3,)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from runners import EnvRunner\n",
    "\n",
    "class DummyPolicy:\n",
    "    def act(self, inputs, training=False):\n",
    "        assert not training\n",
    "        return {\"actions\": np.random.randn(6), \"values\": np.nan}\n",
    "\n",
    "runner = EnvRunner(env, DummyPolicy(), 3,\n",
    "                   transforms=[AsArray()])\n",
    "trajectory = runner.get_next()\n",
    "\n",
    "{k: v.shape for k, v in trajectory.items() if k != \"state\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyhcvAMZNCV5"
   },
   "source": [
    "You will need to implement the following two transformations. \n",
    "\n",
    "The first is `GAE` that implements [Generalized Advantage Estimator](https://arxiv.org/abs/1506.02438).\n",
    "In it you should add two keys to the trajectory: `\"advantages\"` and `\"value_targets\"`. In GAE the advantages\n",
    "$A_t^{\\mathrm{GAE}(\\gamma,\\lambda)}$ are essentially defined as the exponential \n",
    "moving average with parameter $\\lambda$ of the regular advantages \n",
    "$\\hat{A}^{(T)}(s_t) = \\sum_{l=0}^{T-1-t} \\gamma^l r_{t+l} + \\gamma^{T} V^\\pi(s_{T}) - V^\\pi(s_t)$. \n",
    "The exact formula for the computation is the following\n",
    "\n",
    "$$\n",
    "A_{t}^{\\mathrm{GAE}(\\gamma,\\lambda)} = \\sum_{l=0}^{T-1-t} (\\gamma\\lambda)^l\\delta_{t + l}^V, \\, t \\in [0, T)\n",
    "$$\n",
    "where $\\delta_{t+l}^V = r_{t+l} + \\gamma V^\\pi(s_{t+l+1}) - V^\\pi(s_{t+l})$. You can look at the \n",
    "derivation (formulas 11-16) in the paper. Don't forget to reset the summation on terminal\n",
    "states as determined by the flags `trajectory[\"resets\"]`. You can use `trajectory[\"values\"]`\n",
    "to get values of all observations except the most recent which is stored under \n",
    " `trajectory[\"state\"][\"latest_observation\"]`. For this observation you will need to call the policy \n",
    " to get the value prediction.\n",
    "\n",
    "Once you computed the advantages, you can get the targets for training the value function by adding \n",
    "back values:\n",
    "$$\n",
    "\\hat{V}(s_{t+l}) = A_{t+l}^{\\mathrm{GAE}(\\gamma,\\lambda)} + V(s_{t + l}),\n",
    "$$\n",
    "where $\\hat{V}$ is a tensor of value targets that are used to train the value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AC5rUuiKNCV6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "class GAE:\n",
    "    \"\"\" Generalized Advantage Estimator. \"\"\"\n",
    "    def __init__(self, policy, gamma=0.99, lambda_=0.95):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        gamma = self.gamma\n",
    "        lambda_ = self.lambda_\n",
    "        \n",
    "        < insert your code here >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 98432  100 98432    0     0   236k      0 --:--:-- --:--:-- --:--:--  236k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  8320  100  8320    0     0  23771      0 --:--:-- --:--:-- --:--:-- 23703\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  8320  100  8320    0     0  22920      0 --:--:-- --:--:-- --:--:-- 22920\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  272k  100  272k    0     0   581k      0 --:--:-- --:--:-- --:--:--  580k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 16512  100 16512    0     0  46512      0 --:--:-- --:--:-- --:--:-- 46512\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2176  100  2176    0     0   6418      0 --:--:-- --:--:-- --:--:--  6400\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   264  100   264    0     0    760      0 --:--:-- --:--:-- --:--:--   760\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  8320  100  8320    0     0  23771      0 --:--:-- --:--:-- --:--:-- 23771\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  8320  100  8320    0     0  22983      0 --:--:-- --:--:-- --:--:--     0- --:--:-- --:--:-- 22983\n",
      "It's all good!\n"
     ]
    }
   ],
   "source": [
    "def test_gae():\n",
    "    !curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/actions.npy\n",
    "    !curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/log_probs.npy\n",
    "    !curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/values.npy\n",
    "    !curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/observations.npy\n",
    "    !curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/rewards.npy\n",
    "    !curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/resets.npy\n",
    "    !curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/state.npy\n",
    "    !curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/advantages.npy\n",
    "    !curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/value_targets.npy\n",
    "\n",
    "    trajectory = {}\n",
    "    for key in ['actions', 'log_probs', 'values', 'observations', 'rewards', 'resets']:\n",
    "        trajectory[key] = np.load(f'{key}.npy', allow_pickle=True)\n",
    "    trajectory['state'] = {\"latest_observation\": np.load('state.npy')}\n",
    "    \n",
    "    policy = torch.load(f'test_ppo/policy')\n",
    "    gae_to_test = GAE(policy, gamma=0.99, lambda_=0.95)\n",
    "    \n",
    "    gae_to_test(trajectory)\n",
    "    \n",
    "    for key in ['advantages', 'value_targets']:\n",
    "        assert np.allclose(np.load(f'{key}.npy'), trajectory[key])\n",
    "    \n",
    "    print(\"It's all good!\")\n",
    "test_gae()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yce0A8x7NCV7"
   },
   "source": [
    "The main advantage of PPO over simpler policy based methods like A2C is that it is possible\n",
    "to train on the same trajectory for multiple gradient steps. The following class wraps \n",
    "an `EnvRunner`. It should call the runner to get a trajectory, then return minibatches \n",
    "from it for a number of epochs, shuffling the data before each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "J7tIjwbkNCV8"
   },
   "outputs": [],
   "source": [
    "class TrajectorySampler:\n",
    "    \"\"\" Samples minibatches from trajectory for a number of epochs. \"\"\"\n",
    "    def __init__(self, runner, num_epochs, num_minibatches, transforms=None):\n",
    "        self.runner = runner\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_minibatches = num_minibatches\n",
    "        self.transforms = transforms or []\n",
    "        self.minibatch_count = 0\n",
    "        self.epoch_count = 0\n",
    "        self.trajectory = None\n",
    "\n",
    "    def shuffle_trajectory(self):\n",
    "        \"\"\" Shuffles all elements in trajectory.\n",
    "\n",
    "        Should be called at the beginning of each epoch.\n",
    "        \"\"\"\n",
    "        trajectory_len = self.trajectory[\"observations\"].shape[0]\n",
    "\n",
    "        permutation = np.random.permutation(trajectory_len)\n",
    "        for key, value in self.trajectory.items():\n",
    "            if key != 'state':\n",
    "                self.trajectory[key] = value[permutation]\n",
    "\n",
    "    def get_next(self):\n",
    "        \"\"\" Returns next minibatch.  \"\"\"\n",
    "        if not self.trajectory:\n",
    "            self.trajectory = self.runner.get_next()\n",
    "\n",
    "        if self.minibatch_count == self.num_minibatches:\n",
    "            self.shuffle_trajectory()\n",
    "            self.minibatch_count = 0\n",
    "            self.epoch_count += 1\n",
    "\n",
    "        if self.epoch_count == self.num_epochs:\n",
    "            self.trajectory = self.runner.get_next()\n",
    "\n",
    "            self.shuffle_trajectory()\n",
    "            self.minibatch_count = 0\n",
    "            self.epoch_count = 0\n",
    "\n",
    "        trajectory_len = self.trajectory[\"observations\"].shape[0]\n",
    "\n",
    "        batch_size = trajectory_len//self.num_minibatches\n",
    "\n",
    "        minibatch = {}\n",
    "        for key, value in self.trajectory.items():\n",
    "            if key != 'state':\n",
    "                minibatch[key] = value[self.minibatch_count*batch_size: (self.minibatch_count + 1)*batch_size]\n",
    "\n",
    "        self.minibatch_count += 1\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            transform(minibatch)\n",
    "\n",
    "        return minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1UgHPb0NCV8"
   },
   "source": [
    "A common trick to use with GAE is to normalize advantages, the following transformation does that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GZrUlmFYNCV9"
   },
   "outputs": [],
   "source": [
    "class NormalizeAdvantages:\n",
    "    \"\"\" Normalizes advantages to have zero mean and variance 1. \"\"\"\n",
    "    def __call__(self, trajectory):\n",
    "        < insert your code here >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXnare-INCV-"
   },
   "source": [
    "Finally, we can create our PPO runner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tx0Yr0GtNCV_"
   },
   "outputs": [],
   "source": [
    "def make_ppo_runner(env, policy, num_runner_steps=2048,\n",
    "                    gamma=0.99, lambda_=0.95, \n",
    "                    num_epochs=10, num_minibatches=32):\n",
    "    \"\"\" Creates runner for PPO algorithm. \"\"\"\n",
    "    runner_transforms = [AsArray(),\n",
    "                       GAE(policy, gamma=gamma, lambda_=lambda_)]\n",
    "    runner = EnvRunner(env, policy, num_runner_steps, \n",
    "                     transforms=runner_transforms)\n",
    "\n",
    "    sampler_transforms = [NormalizeAdvantages()]\n",
    "    sampler = TrajectorySampler(runner, num_epochs=num_epochs, \n",
    "                              num_minibatches=num_minibatches,\n",
    "                              transforms=sampler_transforms)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX1aZueWNCWA"
   },
   "source": [
    "In the next cell you will need to implement Proximal Policy Optimization algorithm itself. The algorithm\n",
    "modifies the typical policy gradient loss in the following way:\n",
    "\n",
    "$$\n",
    "J_{\\pi}(s, a) = \\frac{\\pi_\\theta(a|s)}{\\pi_\\theta^{\\text{old}}(a|s)} \\cdot A^{\\mathrm{GAE}(\\gamma,\\lambda)}(s, a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{\\pi}^{\\text{clipped}}(s, a) = \\mathrm{clip}\\left(\n",
    "\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta^{\\text{old}}}(a|s)},\n",
    "1 - \\text{cliprange}, 1 + \\text{cliprange}\\right)\\cdot A^{\\mathrm{GAE(\\gamma, \\lambda)}}(s)\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{\\text{policy}} = -\\frac{1}{T}\\sum_{l=0}^{T-1}\\min\\left(J_\\pi(s_{t + l}, a_{t + l}), J_{\\pi}^{\\text{clipped}}(s_{t + l}, a_{t + l})\\right).\n",
    "$$\n",
    "\n",
    "The value loss is also modified:\n",
    "\n",
    "$$\n",
    "L_{V}^{\\text{clipped}} = \\frac{1}{T}\\sum_{l=0}^{T-1} \\max(l^{simple}(s_{t + l}), l^{clipped}(s_{t + l}))\n",
    "$$\n",
    ", where $l^{simple}$ is your standard critic loss\n",
    "$$\n",
    "l^{simple}(s_{t + l}) = [V_\\theta(s_{t+l}) - G(s_{t + l})]^2\n",
    "$$\n",
    "\n",
    "and $l^{clipped}$ is a clipped version that limits large changes of the value function:\n",
    "$$\n",
    "l^{clipped}(s_{t + l}) = [\n",
    "V_{\\theta^{\\text{old}}}(s_{t+l}) +\n",
    "\\text{clip}\\left(\n",
    "V_\\theta(s_{t+l}) - V_{\\theta^\\text{old}}(s_{t+l}),\n",
    "-\\text{cliprange}, \\text{cliprange}\n",
    "\\right) - G(s_{t + l})] ^ 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JYGV4EmxNCWB"
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, policy, optimizer,\n",
    "               cliprange=0.2,\n",
    "               value_loss_coef=0.25,\n",
    "               max_grad_norm=0.5):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.cliprange = cliprange\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        # Note that we don't need entropy regularization for this env.\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def policy_loss(self, trajectory, act):\n",
    "        \"\"\" Computes and returns policy loss on a given trajectory. \"\"\"\n",
    "        < insert your code here >\n",
    "\n",
    "    def value_loss(self, trajectory, act):\n",
    "        \"\"\" Computes and returns value loss on a given trajectory. \"\"\"\n",
    "        < insert your code here >\n",
    "\n",
    "    def loss(self, trajectory):\n",
    "        act = self.policy.act(trajectory[\"observations\"], training=True)\n",
    "        policy_loss = self.policy_loss(trajectory, act)\n",
    "        value_loss = self.value_loss(trajectory, act)\n",
    "\n",
    "        return policy_loss + self.value_loss_coef * value_loss\n",
    "\n",
    "    def step(self, trajectory):\n",
    "        \"\"\" Computes the loss function and performs a single gradient step. \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss(trajectory)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = nn.utils.clip_grad_norm_(self.policy.model.parameters(), self.max_grad_norm)\n",
    "\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVNS0IFhNCWB"
   },
   "source": [
    "Now everything is ready to do training. In one million of interactions it should be possible to \n",
    "achieve the total raw reward of about 1500. You should plot this quantity with respect to \n",
    "`runner.step_var` &mdash; the number of interactions with the environment. It is highly \n",
    "encouraged to also provide plots of the following quantities (these are useful for debugging as well):\n",
    "\n",
    "* [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) between \n",
    "value targets and value predictions\n",
    "* Entropy of the policy $\\pi$\n",
    "* Value loss\n",
    "* Policy loss\n",
    "* Value targets\n",
    "* Value predictions\n",
    "* Gradient norm\n",
    "* Advantages\n",
    "\n",
    "For optimization it is suggested to use Adam optimizer with linearly annealing learning rate \n",
    "from 3e-4 to 0 and epsilon 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WmroaOHX33nZ"
   },
   "outputs": [],
   "source": [
    "model = PolicyModel()\n",
    "model = model.cuda()\n",
    "\n",
    "policy = Policy(model)\n",
    "\n",
    "runner = make_ppo_runner(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Psvv8oru3POG"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "2W5StJsb1mz2",
    "outputId": "7a902763-0cc7-4dfe-9746-3511fbe555f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5399/250000 [01:26<1:05:08, 62.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a73349c603f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \"\"\"\n\u001b[1;32m    352\u001b[0m     \u001b[0m_warn_if_gui_out_of_main_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backend_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             display(\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2191\u001b[0m                            else suppress())\n\u001b[1;32m   2192\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2193\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m                     bbox_inches = self.figure.get_tightbbox(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   1864\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2745\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2747\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n\u001b[0m\u001b[1;32m   1166\u001b[0m                                                                 renderer)\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1092\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1093\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1092\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1093\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m             \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mclean_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mismath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_math\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclean_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 w, h, d = renderer.get_text_width_height_descent(\n\u001b[0m\u001b[1;32m    294\u001b[0m                     clean_line, self._fontproperties, ismath=ismath)\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hinting_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_agg_font\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mfont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_width_height\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# width and height of unrotated string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_get_agg_font\u001b[0;34m(self, prop)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \"\"\"\n\u001b[1;32m    272\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindfont\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_font\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mfont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/matplotlib/font_manager.py\u001b[0m in \u001b[0;36mget_font\u001b[0;34m(filename, hinting_factor)\u001b[0m\n\u001b[1;32m   1412\u001b[0m     \u001b[0;31m# Resolving the path avoids embedding the font twice in pdf/ps output if a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \u001b[0;31m# single font is selected using two different relative paths.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhinting_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0mhinting_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text.hinting_factor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/posixpath.py\u001b[0m in \u001b[0;36mrealpath\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    389\u001b[0m symbolic links encountered in the path.\"\"\"\n\u001b[1;32m    390\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_joinrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/posixpath.py\u001b[0m in \u001b[0;36m_joinrealpath\u001b[0;34m(path, rest, seen)\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpardir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mnewpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mislink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     75\u001b[0m     ends with a separator.\"\"\"\n\u001b[1;32m     76\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/posixpath.py\u001b[0m in \u001b[0;36m_get_sep\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34mb'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(policy.model.parameters(), lr = 3e-4, eps=1e-5)\n",
    "epochs = 250000\n",
    "\n",
    "lr_mult = lambda epoch: (1 - (epoch/epochs))\n",
    "sched = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_mult)\n",
    "\n",
    "ppo = PPO(policy, optimizer)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    trajectory = runner.get_next()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        clear_output(True)\n",
    "        rewards = np.array(env.env.episode_rewards)\n",
    "\n",
    "        if rewards.size > 0:\n",
    "            plt.plot(rewards[:, 0], rewards[:, 1], label = \"episode rewards\")\n",
    "            plt.title(\"Reward\")\n",
    "            plt.xlabel(\"Total steps\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "\n",
    "    ppo.step(trajectory)\n",
    "    sched.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "OxD3w7uWbsYt",
    "outputId": "551690d0-f96d-406a-efc7-73ff50508c70"
   },
   "outputs": [],
   "source": [
    "rewards = np.array(env.env.episode_rewards)\n",
    "\n",
    "if rewards.size > 0:\n",
    "    plt.plot(rewards[:, 0], rewards[:, 1], label = \"episode rewards\")\n",
    "    plt.title(\"Reward\")\n",
    "    plt.xlabel(\"Total steps\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.ylim(-1000, 2500)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sixQI_jMASJR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"Копия блокнота \"Копия блокнота \"Копия блокнота \"ppo.ipynb\"\"\"\"",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
